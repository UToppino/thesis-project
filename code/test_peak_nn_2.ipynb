{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peak method applied to NN\n",
    "The script takes the ECG datasets and for every sample extracts relevant peaks which are then used to train the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import keras\n",
    "import keras_tuner as kt\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Convolution1D, Convolution2D, MaxPool1D, Flatten, Dropout, AveragePooling1D\n",
    "from keras.layers import Input\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.utils import resample\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from file and generate training, test and validation sets\n",
    "\n",
    "DATA_SPLIT = 0.1\n",
    "SEED = 12345\n",
    "\n",
    "# Read files\n",
    "train_df=pd.read_csv('src/mitbih_train.csv',header=None)\n",
    "test_df=pd.read_csv('src/mitbih_test.csv',header=None)\n",
    "\n",
    "train_df_x = train_df.iloc[:,:186].values\n",
    "train_df_y = train_df[187]\n",
    "set_train, set_valid, target_train, target_valid = train_test_split(train_df_x, train_df_y, test_size=DATA_SPLIT, random_state=SEED)\n",
    "#train_df, valid_df = train_test_split(train_df, test_size=DATA_SPLIT, random_state=SEED)\n",
    "\n",
    "set_test=test_df.iloc[:,:186].values\n",
    "target_test=test_df[187]\n",
    "\n",
    "y_train=to_categorical(target_train)\n",
    "y_test=to_categorical(target_test)\n",
    "y_valid=to_categorical(target_valid)\n",
    "\n",
    "num_train = len(y_train)\n",
    "num_test = len(y_test)\n",
    "num_valid = len(y_valid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separate signal data from category\n",
    "\n",
    "target_train=train_df[187]\n",
    "target_test=test_df[187]\n",
    "target_valid=valid_df[187]\n",
    "y_train=to_categorical(target_train)\n",
    "y_test=to_categorical(target_test)\n",
    "y_valid=to_categorical(target_valid)\n",
    "set_train=train_df.iloc[:,:186].values\n",
    "set_test=test_df.iloc[:,:186].values\n",
    "set_valid=valid_df.iloc[:,:186].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine train and test data for preprocessing\n",
    "\n",
    "v_data = np.append(set_train,set_test, 0)\n",
    "v_data = np.append(v_data, set_valid, 0)\n",
    "v_result = np.append(y_train, y_test, 0)\n",
    "v_result = np.append(v_result, y_valid, 0)\n",
    "print(len(v_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define time array\n",
    "\n",
    "v_time = np.arange(0, len(v_data[0]), 1, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate moving averages\n",
    "\n",
    "# Window size for moving averages\n",
    "LOWPASS_WINDOW = 3\n",
    "\n",
    "v_data_lp = np.zeros_like(v_data)\n",
    "#tmp_lowpass = np.zeros_like(v_data)\n",
    "\n",
    "def moving_average(a, n=3):\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n\n",
    "\n",
    "for i in range(0, len(v_data)):\n",
    "    tmp_lowpass = moving_average(v_data[i], LOWPASS_WINDOW)\n",
    "    v_data_lp[i] = np.pad(tmp_lowpass, (LOWPASS_WINDOW-1,0), 'constant', constant_values=(tmp_lowpass[0],0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zeros array for x-axis\n",
    "\n",
    "axis_x = np.zeros_like(v_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify potential peaks\n",
    "# Compare 3 signal samples S0, S1, S2\n",
    "# Peak if:  S1>=S0 and S1>S2\n",
    "#           S1<=S0 and S1<S2\n",
    "\n",
    "DELTA_PEAK = 0.03       # Minimum signal amplitude from one peak to the next\n",
    "DELTA_TIME = 35         # Maximum time after a peak in which DELTA_PEAK is considered (after this time peaks with any amplitude are considered)\n",
    "\n",
    "v_peaks_data = np.zeros_like(v_data_lp)\n",
    "v_peaks_time = np.zeros_like(v_data_lp)\n",
    "#j = 0\n",
    "\n",
    "for n in range(len(v_data_lp)):\n",
    "    j = 0\n",
    "    # Walk through all samples\n",
    "    for i in range(1, len(v_data_lp[n])-1):\n",
    "        # Identify peak\n",
    "        if  ((v_data_lp[n][i] >= v_data_lp[n][i-1]) and (v_data_lp[n][i] > v_data_lp[n][i+1])) or \\\n",
    "            ((v_data_lp[n][i] <= v_data_lp[n][i-1]) and (v_data_lp[n][i] < v_data_lp[n][i+1])):\n",
    "            # Check if peak value amd time\n",
    "            if  (abs(v_data_lp[n][i] - v_peaks_data[n][j-1]) > DELTA_PEAK) or \\\n",
    "                (abs(v_time[i] - v_peaks_time[n][j-1]) > DELTA_TIME):\n",
    "                #print(\"n:\", str(n), \" i:\", str(i), \" j:\", str(j))\n",
    "                v_peaks_data[n][j] = v_data_lp[n][i]\n",
    "                v_peaks_time[n][j] = v_time[i]\n",
    "                j += 1\n",
    "\n",
    "#v_peaks_data[n] = v_peaks_data[n][v_peaks_time[n] != 0]\n",
    "#v_peaks_time[n] = v_peaks_time[n][v_peaks_time[n] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete samples with too many peaks\n",
    "\n",
    "MAX_PEAK_NUM = 240       # Maximum peaks for each array\n",
    "\n",
    "n_offset = 0\n",
    "delta_num_train = 0\n",
    "delta_num_test = 0\n",
    "delta_num_valid = 0\n",
    "del_peak_cat = [0,0,0,0,0]      # Log the category of each deleted peak\n",
    "\n",
    "print('Samples before peak check: ' + str(len(v_peaks_data)))\n",
    "\n",
    "for n in range(len(v_peaks_data)):\n",
    "    index = n-n_offset\n",
    "    if len(v_peaks_data[index][v_peaks_time[index] != 0]) > MAX_PEAK_NUM:\n",
    "        # Delete from peak set\n",
    "        v_peaks_data = np.delete(v_peaks_data, index, 0)\n",
    "        v_peaks_time = np.delete(v_peaks_time, index, 0)\n",
    "        # Delete from category set\n",
    "        del_peak_cat = del_peak_cat + v_result[index]\n",
    "        v_result = np.delete(v_result, index, 0)\n",
    "        # Delete from samples set\n",
    "        v_data_lp = np.delete(v_data_lp, index, 0)\n",
    "        \n",
    "        n_offset += 1\n",
    "        if n < num_train:\n",
    "            delta_num_train += 1\n",
    "        elif n < num_train+num_test:\n",
    "            delta_num_test += 1\n",
    "        else:\n",
    "            delta_num_valid += 1\n",
    "\n",
    "num_train -= delta_num_train\n",
    "num_test -= delta_num_test\n",
    "num_valid -= delta_num_valid\n",
    "\n",
    "print('Samples after peak check: ' + str(len(v_peaks_data)))\n",
    "print('Categories: ')\n",
    "print(del_peak_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "N_ROW = 5\n",
    "N_COL = 4\n",
    "\n",
    "fig, axs = plt.subplots(N_ROW, N_COL)\n",
    "fig.set_size_inches(15, 2*N_ROW)\n",
    "fig.suptitle(\"Features\")\n",
    "for i, ax in enumerate(axs.flatten()):\n",
    "    ax.title.set_text('sample_'+str(i))\n",
    "    ax.plot(axis_x, linewidth=0.5, color=\"black\")\n",
    "    ax.plot(v_data[i], linewidth=0.5, label=\"original\")\n",
    "    ax.plot(v_time, v_data_lp[i], label=\"low pass\")\n",
    "\n",
    "    v_peaks_data_plot = v_peaks_data[i][v_peaks_time[i] != 0]\n",
    "    v_peaks_time_plot = v_peaks_time[i][v_peaks_time[i] != 0]\n",
    "    ax.plot(v_peaks_time_plot, v_peaks_data_plot, 'mo', label=\"peaks\")\n",
    "    ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create array combining peak amplitudes and peak times\n",
    "\n",
    "# Find maximum length of peaks arrays\n",
    "peak_length = np.zeros(len(v_peaks_data))\n",
    "for i in range(len(v_peaks_data)):\n",
    "    peak_length[i] = len(v_peaks_data[i][v_peaks_time[i] != 0])\n",
    "\n",
    "max_length = int(np.max(peak_length))\n",
    "peak_avg = np.ones(len(peak_length))*np.mean(peak_length)\n",
    "\n",
    "\n",
    "# Define structure of arrays\n",
    "input_peaks = np.zeros((len(v_peaks_data),2*max_length))\n",
    "\n",
    "# Fill strcuture with peaks data\n",
    "# P0,dT0,P1,dT1,P2,dT2...,Pn,Tn  \n",
    "# with Px: peak amplitude, Tx: delta time from previous peak\n",
    "for n in range(len(v_peaks_data)):\n",
    "    prev_time = 0\n",
    "    tmp_data = v_peaks_data[n][v_peaks_time[n] != 0]\n",
    "    tmp_time = v_peaks_time[n][v_peaks_time[n] != 0]\n",
    "    for i in range(0,len(tmp_data)):\n",
    "        # Copy peaks to structure\n",
    "        input_peaks[n][2*i] = tmp_data[i]\n",
    "        input_peaks[n][2*i+1] = tmp_time[i] - prev_time\n",
    "        prev_time = tmp_time[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot peaks data for all samples\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (15,5)\n",
    "plt.title('Peaks each sample')\n",
    "plt.plot(peak_length, label=\"peaks/sample\")\n",
    "plt.plot(peak_avg, 'r--', linewidth=0.5, label='avg peaks')\n",
    "plt.legend()\n",
    "plt.show\n",
    "#print(v_data[max_sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot samples with high number of peaks\n",
    "N_ROW = 5\n",
    "N_COL = 4\n",
    "N_TOT = N_ROW*N_COL\n",
    "\n",
    "max_index = np.argpartition(peak_length, -N_TOT)[-N_TOT:]\n",
    "\n",
    "fig, axs = plt.subplots(N_ROW, N_COL)\n",
    "fig.set_size_inches(15, 15)\n",
    "fig.suptitle(\"Features\")\n",
    "for n, ax in enumerate(axs.flatten()):\n",
    "    i = max_index[n]\n",
    "    ax.title.set_text('sample_'+str(i))\n",
    "    ax.plot(axis_x, linewidth=0.5, color=\"black\")\n",
    "    ax.plot(v_data_lp[i], label=\"low pass\")\n",
    "\n",
    "    v_peaks_data_plot = v_peaks_data[i][v_peaks_time[i] != 0]\n",
    "    v_peaks_time_plot = v_peaks_time[i][v_peaks_time[i] != 0]\n",
    "    ax.plot(v_peaks_time_plot, v_peaks_data_plot, 'm.', label=\"peaks\")\n",
    "    #ax.plot(peak_length[i]*np.ones_like(v_data_lp[i]), 'r--', linewidth=0.5)\n",
    "    ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate train and test data after processing\n",
    "\n",
    "x_train = input_peaks[range(0, num_train, 1)]\n",
    "x_test = input_peaks[range(num_train, num_train+num_test, 1)]\n",
    "x_valid = input_peaks[range(num_train+num_test, num_train+num_test+num_valid, 1)]\n",
    "\n",
    "y_train = v_result[range(0, num_train, 1)]\n",
    "y_test = v_result[range(num_train, num_train+num_test, 1)]\n",
    "y_valid = v_result[range(num_train+num_test, num_train+num_test+num_valid, 1)]\n",
    "\n",
    "#print(v_result)\n",
    "#print(x_train)\n",
    "#y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Architecture Search with Tensorflow\n",
    "\n",
    "def build_model(hp):\n",
    "  model = keras.Sequential()\n",
    "  \n",
    "  # Standard input layer\n",
    "  model.add(keras.layers.Input(shape=((input_peaks[0].shape[0],1)), name='inputs_cnn'))\n",
    "\n",
    "  # Tune whether to use convolutional layer\n",
    "  if hp.Boolean(\"conv_1\"):\n",
    "    model.add(keras.layers.Convolution1D( filters=hp.Int(\"filters_1\", min_value=1, max_value=5, step=1), \n",
    "                                          kernel_size=hp.Int(\"kernel_size_1\", min_value=3, max_value=7, step=2),\n",
    "                                          strides=1, padding=\"same\", activation='relu'))\n",
    "  \n",
    "  if hp.Boolean(\"maxpool_1\"):\n",
    "    model.add(keras.layers.MaxPool1D(pool_size=hp.Int(\"pool_size_1\", min_value=1, max_value=6, step=1)))\n",
    "\n",
    "  if hp.Boolean(\"conv_2\"):\n",
    "    model.add(keras.layers.Convolution1D( filters=hp.Int(\"filters_2\", min_value=1, max_value=5, step=1), \n",
    "                                          kernel_size=hp.Int(\"kernel_size_2\", min_value=3, max_value=7, step=2),\n",
    "                                          strides=1, padding=\"same\", activation='relu'))\n",
    "    \n",
    "  if hp.Boolean(\"maxpool_2\"):\n",
    "    model.add(keras.layers.MaxPool1D(pool_size=hp.Int(\"pool_size_2\", min_value=1, max_value=6, step=1)))\n",
    "  \n",
    "  # Flatten before dense layers\n",
    "  model.add(keras.layers.Flatten())\n",
    "\n",
    "  # Tune the number of units in the first Dense layer\n",
    "  if hp.Boolean(\"dense_1\"):\n",
    "    model.add(keras.layers.Dense( units=hp.Int(\"units_1\", min_value=5, max_value=20, step=1),\n",
    "                                  activation=\"relu\"))\n",
    "    \n",
    "  #if hp.Boolean(\"dense_2\"):\n",
    "  #  model.add(keras.layers.Dense( units=hp.Int(\"units_2\", min_value=5, max_value=20, step=1),\n",
    "  #                                activation=\"relu\"))\n",
    "  \n",
    "  model.add(keras.layers.Dense(5, activation='softmax', name='main_output'))\n",
    "\n",
    "  # Tune the learning rate \n",
    "  learning_rate = hp.Float(\"lr\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n",
    "  model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    "  )\n",
    "\n",
    "  return model\n",
    "\n",
    "build_model(kt.HyperParameters())\n",
    "\n",
    "tuner = kt.RandomSearch(\n",
    "    hypermodel=build_model,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_trials=15,\n",
    "    executions_per_trial=2,\n",
    "    overwrite=True,\n",
    "    directory=\"my_dir\",\n",
    "    project_name=\"helloworld\",\n",
    ")\n",
    "\n",
    "tuner.search(x_train, y_train, epochs=4, validation_data=(x_valid, y_valid))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 2 models.\n",
    "models = tuner.get_best_models(num_models=2)\n",
    "best_model = models[0]\n",
    "best_model.summary()\n",
    "\n",
    "model = best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "EPOCH_NUM = 20\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Train model\n",
    "history=model.fit(x_train, y_train, epochs=EPOCH_NUM, batch_size=BATCH_SIZE, validation_data=(x_valid,y_valid))\n",
    "#history=model.fit(x_train, y_train, epochs=EPOCH_NUM, batch_size=BATCH_SIZE, validation_data=(x_valid,y_valid), callbacks=[tensorboard_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prediction = model.predict(x_test)\n",
    "y_prediction = np.argmax (y_prediction, axis = 1)\n",
    "y_test_pred=np.argmax(y_test, axis=1)\n",
    "\n",
    "# Create confusion matrix and normalize it over predicted (columns)\n",
    "#result = confusion_matrix(y_test, y_prediction , normalize='pred')\n",
    "result = confusion_matrix(y_test_pred, y_prediction , normalize='true')\n",
    "classes = [0,1,2,3,4]\n",
    "result_norm = np.around(result, 4)\n",
    "mat_result = pd.DataFrame(result_norm,\n",
    "                     index = classes, \n",
    "                     columns = classes)\n",
    "\n",
    "total_pred = np.trace(result) / 5\n",
    "\n",
    "print('Confusion matrix')\n",
    "print(mat_result)\n",
    "print()\n",
    "print('Average accuracy')\n",
    "print(total_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(4, 4))\n",
    "sns.heatmap(mat_result, annot=True,cmap=plt.cm.Blues)\n",
    "#plt.title(model_name)\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.tight_layout()\n",
    "#plt.savefig(log_dir + '/' + model_name + '.png', pad_inches=0.1)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
